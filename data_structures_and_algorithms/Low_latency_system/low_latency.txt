to study : 

        concurrency, multithreading and intricacies of the c++ memory model
        efficient modern c++ -scott meyers
        C++ concurrency : Anthony Williams

        networking and real time systems 

        networking : understand the basics of networking and how network latency can impact system performance 

        real time systems : study real time systems RTOS and their principles 
        
        specialized low latency concepts : read designing data intensive applications by Martin Kleppmann and whitepapers on
        low latency system design 

        high frequency trading : look into resource specific to high frequency trading systems if thats your area of interest

        Practical Experience:

        Projects: 
            Work on projects that require low latency, such as real-time data processing or networked applications.
            Open Source: Contribute to open-source projects focused on performance optimization.
            Optimization: Learn and practice performance profiling and optimization tools like perf, valgrind, and gprof.

        Specific Techniques:

            Lock-Free Programming: Study and practice lock-free and wait-free algorithms.
            Memory Management: Understand custom memory allocators and techniques to minimize cache misses.
            Asynchronous Programming: Learn about asynchronous I/O and event-driven programming models.

        Join Professional Communities:

            Engage with professional communities and forums focused on low-latency systems and C++.
            Attend conferences and workshops to stay updated on the latest techniques and technologies.



Low Latency System Development 

            main:
                .LFB1
                    Movl    $100, %edi
                    Call    _Z9factorialj



This code snippet demonstrates assembly language, specifically in AT&T-syntax often used with the GNU assembler.
 It invokes a function to compute the factorial of a number (in this case, 100).Here's a brief explanation of the components:
 1. 
 "main" is the function from where the execution of program starts in most programming languages.
 2. 
 ".LFB1" is a label automatically generated by the GNU compiler during assembly generation to represent a function in the assembly code.
 3. 
 "Movl $100, %edi" is an instruction that moves the value 100 into the %edi register. In this context, 
 %edi register is commonly used to hold the first integer or pointer argument in a function call in SystemV ABI. 
 
 4. 
 "Call _Z9factorialj" is a statement that calls the function "factorial." The strange name "_Z9factorialj" is "mangled" by the C++ compiler 
 to incorporate information about function arguments and return type. This is where the factorial operation would be performed on 
 the value located in the %edi register (which is currently 100). Remember that this is x86_64 assembly with SysV ABI.In summary, 
 this code computes the factorial of 100 and this operation is performed in the assembly language.


INTRODUCING C++ CONCEPTS AND EXPLORING IMPORTANT LOW LATENCY applications



Designing Some Common Low Latency Applications in C++


in this we shall introduce the electronic trading system since we shall use this alot in the rest of the book to build system from 
scratch in c++ with a focus on using low latency ideas 

in this chapter we shall investigate the useful role that low latency systems have in systems such as real time video streaming,
offline and online gaming applications, iot machines and applications and electronic trading systems 



Understanding low latency performance in live video streaming applications

Defining important concepts in low latency streaming 

atency in general refers to the time delay between an input event and the output event.
In the context of live video streaming applications, latency refers specifically to the time from when a live video stream hits the camera on the 
recording device and then gets transported to the target audience’s screens and gets rendered and displayed there.

Video distribution services and content delivery network

VDS (video distribution service)- basically means the system responsible for taking multiple incoming streams of video and audio from the sources and presenting them to 
the viewers. such an example would the content delivery network(CDN) which is a means of efficiently distributing context across the globe.


Transcoding refers to the process of decoding a media stream from one format (so lower-level details such as codec, video size, sampling rates, 
encoder formats, etc.) and possibly recoding it in a different format or with different parameters.
Transmuxing is similar to transcoding but here, the delivery format changes without any changes to the encoding, as in the case of transcoding.
Transrating is also similar to transcoding, but we change the video bitrate; usually, it is compressed to a lower value. 
The video bitrate is the number of bits (or kilobits) being transferred per second and captures the information and quality in the video stream.


Understanding sources of latency in video streaming applications 

startup latency and the latency in video frames once the transmission starts 


describing possibilities of high latencies in the path 

physical distance, server load and the internet quality 
capture equipment and hardware
streaming protocol, transmission and jitter buffer 
encoding - transcoding and transrating 


Impact of high latencies 

low audio video quality 
buffering pauses and delays
audio video syncronization issues 
playback rewinding and fast forwarding 



Exploring technologies for low latency video streaming 

non-HTTP-based protocols
RTSP
FLASH and RTMP
HTTP based protocols such as 
            HTTP live streaming HLS
            HTTP dynamic streaming HDS
            Microsoft smooth streaming MSS 
            Dynamic Adaprive Streaming over HTTP (DASH)
            Common media application format CMAF
            High Efficienncy stream protocols HESP


SOME OF THE SOLUTIONS AND PLATFORMS FOR LOW LATENCY Streaming

Twitch
zoom 
dacast
ant media server 
vimeo 
wowza
evercast 
cachefly
vonage video API
Open broadcast software



understanding low latency constraints matters in gaming applications 

ping
fps
refresh rate 
input lag
response time 
network bandwidth 
network packet loss and jitter 
networking protocol


Improving gaming application performance 

from a developers perspective the above come to play

Managing memory, optimizing cache acess and optimization of the hot path
Frustum culling
Caching calculations and using mathematical approximations
Prioritizing critical tasks and leveraging CPU idle time
Ordering draw calls depending on layer, depth, and texture



Exploring low latency electronic trading

exploring ultra low latency trading also known as HFT. we will build a full end to end low latency electronic trading system from scratch in c++,
we shall explore what is needed to develop the high frequency trading system from scratch

this will be the market making and liquidity taking algorithm 

a market making algorithm has orders in the market that other participants can trade against when neeeded. a market making algorithm thus needs to
constantly re evaluate its active orders and change the prices and quantitites for them depending on the market conditions 

a liquidity taking algorithm however does not always have active orders in the market. this algorithm instead waits for an opportunity to present itself
against a market making algorithms active orders in the book

A simple view of the HFT market would be a constant battle between market-making and liquidity-taking algorithms because they naturally take opposite sides.

In this setup, a market-making algorithm loses money when it is slow at modifying its active orders in the market. For instance, say depending on market conditions, 
it is quite clear that the market prices are going to go up in the short term; a market-making algorithm will try to move or cancel its sell orders if they are at risk 
of being executed since it no longer wants to sell at those prices. A liquidity-taking algorithm, at the same time, will try to see whether it can send a buy order to 
trade against a market maker’s sell order at that price. In this race, if the market-making algorithm is slower than the liquidity-taking algorithms,
 it will not be able to modify or cancel its sell order. If the liquidity-taking algorithm is slow, it will not be able to execute against the orders it wanted 
 to either because a different (and faster) algorithm was able to execute before it or because the market maker was able to move out of the way. 
 This example should make it clear to you that latency directly affects trading revenue in electronic trading.


 achieving low latencies in electronic trading 

 Optimizing trading server hardware
 Network Interface Cards, switches, and kernel bypass
 Understanding multithreading, locks, context switches, and CPU scheduling
 Dynamically allocating memory and managing memory
 Static versus dynamic linking and compile time versus runtime




 Low latency application development in c++


 we shall understand what metrics matter for latency sensitive applications 

 difference between latency sensitive vs latency critical applications 

 latency sensitive is an application in which performance latencies are reduced,which in latter effect improves the business impact and profitability 
 in this case the system may be still profitable at higher performance latencies but can also be significantly more profitable if the latencies 
 are reduced 
 eg operating system, web browser, databases 


 latency critical applications on the other hand is one that will fail completely is the latency is higher than a certain threshold
 eg, traffic control system, autonomous vehicles and some medical appliances 


 Measuring latency



 we can use different unit of measuring latency eg cpu clock cycles or time 

 Time to first byte

Time to first byte is measured as the time elapsed from when the sender sends the first byte of a request (or response) to the moment when the receiver receives 
the first byte. This typically (but not necessarily) applies to network links or systems where there are data transfer operations that are latency-sensitive. 


Round-trip time

Round-trip time (RTT) is the sum of the time it takes for a packet to travel from one process to another and then the time it takes for the response packet to reach
the original process. Again, this is typically (but not necessarily) used for network traffic going back and forth between server and client processes, but can also 
be used for two processes communicating in general.


Tick-to-trade

Tick-to-trade (TTT) is similar to RTT and is a term most commonly used in electronic trading systems. TTT is defined as the time from when a packet 
(usually a market data packet) first hits a participant’s infrastructure (trading server) to the time when the participant is done processing the packet and sends a 
packet out (order request) to the trading exchange. So, TTT includes the time spent by the trading infrastructure to read the packet, process it, calculate trading signals, 
generate an order request in reaction to that, and put it on the wire. Putting it on the wire typically means writing something to a network socket. We will revisit this 
topic and explore it in greater 


CPU clock cycles

CPU clock cycles are basically the smallest increment of work that can be done by the CPU processor. In reality, they are the amount of time between two pulses of the 
oscillator that drives the CPU processor. Measuring CPU clock cycles is typically used to measure latency at the instruction level – that is, at an extremely low level 
at the processor level. C++ is both a low-level as well as a high-level language; it lets you get as close to the hardware as needed and also provides higher-level 
abstractions such as classes, templates, and so on. But generally, C++ developers do not spend a lot of time dealing with extremely low-level or possibly assembly code. 
This means that the compiled machine code might not be exactly what a C++ developer expects. Additionally, depending on the compiler versions, the processor architectures, 
and so on, there may be even more sources of differences. So, for extremely performance-sensitive low latency code, it is often not uncommon for engineers to measure how 
many instructions are executed and how many CPU clock cycles are required to do so. This level of optimization is typically the highest level of optimization possible, 
alongside kernel-level optimizations.


Differenciating between latency metrics 

The relative importance of a specific latency metric over the other depends on the application and the business itself. As an example, a latency-critical application 
such as an autonomous vehicle software system cares about peak latency much more than the mean latency. Low latency electronic trading systems typically care more about 
mean latency and smaller latency variance than they do about peak latency. Video streaming and playback applications might generally prioritize high throughput over lower 
latency variance due to the nature of the application and the consumers.


throughput versus latency 

throughput - defined as how much work gets done in a certain period of time
to improve throughput the ussual approach usually is : 
introduce parallelism and add additional computing memory and networking resources 

kindly note that this may not increase latency as latency is how fast a single task is completed 

latency - how quickly a single task is completed 



Mean Latency

Mean latency is basically the expected average response time of a system. It is simply the average of all the latency measurement observations.
 This metric includes large outliers, so can be a noisy metric for systems that experience a large range of performance latencies.

Median latency

Median latency is typically a better metric for the expected response time of a system. Since it is the median of the latency measurement 
observations, it excludes the impact of large outliers. Due to this, it is sometimes preferred over the mean latency metric.


Peak latency

Peak latency is an important metric for systems where a single large outlier in performance can have a devastating impact on the system. 
Large values of peak latency can also significantly influence the mean latency metric of the system.


Latency variance

For systems that require a latency profile that is as deterministic as possible, the actual variance of the performance latency is an important metric. 
This is typically important where the expected latencies are quite predictable. For systems with low latency variance, the mean, median, and peak latencies 
are all expected to be quite close to each other.


Requirement of latency sensitive applications 
low latencies on average
capped at peak latency
Predictable latency – low latency variance
high throughput


understanding why C++ is the preffered language

Compiled language
closer to hardware - low level language 
deterministic usage of resources 
speed and high performance :- excellent concurrency and multithreading support 
                           :- supports features such as macros and pre processor directives, a constexpr specifier and template metaprogramming which helps
                           move a large part of the processing from runtime to compile time 


language constructs and features 

Portability
compiler optimization
statically typed 
multiple paradigms 
libraries 
standard template library (stl)
boost
abstractions
GNU specific library
Active Template library(ATL)
Eigen
LAPACK
OpenCV
mlpack
QT
crypto ++


